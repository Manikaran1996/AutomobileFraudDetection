{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('formImage.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePadding(image):\n",
    "    up = 0\n",
    "    down = 0\n",
    "    left = 0\n",
    "    right = 0\n",
    "    while sum(image[:,0] == 0) > 0.95*image.shape[0] or sum(image[:,0] == 255) > 0.95*image.shape[0]:\n",
    "        image = image[:,1:]\n",
    "        up += 1\n",
    "    while sum(image[0,:] == 0) > 0.95*image.shape[1] or sum(image[0,:] == 255) > 0.95*image.shape[1]:\n",
    "        image = image[1:,:]\n",
    "        left += 1\n",
    "    while sum(image[:,-1] == 0) > 0.95*image.shape[0] or sum(image[:,-1] == 255) > 0.95*image.shape[0]:\n",
    "        image = image[:,:-1]\n",
    "        right += 1\n",
    "    while sum(image[-1,:] == 0) > 0.95*image.shape[1] or sum(image[-1,:] == 255) > 0.95*image.shape[1]:\n",
    "        image = image[:-1,:]\n",
    "        down += 1\n",
    "    print('Number of lines removed-\\nUp = {}\\nLeft = {}\\nRight = {}\\nDown = {}'.format(up, left, right, down))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePadding2(image):\n",
    "    while sum(image[:,0] == 0) == image.shape[0] or sum(image[:,0] == 255) == image.shape[0]:\n",
    "        image = image[:,1:]\n",
    "    while sum(image[0,:] == 0) == image.shape[1] or sum(image[0,:] == 255) == image.shape[1]:\n",
    "        image = image[1:,:]\n",
    "    while sum(image[:,-1] == 0) == image.shape[0] or sum(image[:,-1] == 255) == image.shape[0]:\n",
    "        image = image[:,:-1]\n",
    "    while sum(image[-1,:] == 0) == image.shape[1] or sum(image[-1,:] == 255) == image.shape[1]:\n",
    "        image = image[:-1,:]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBoxes(image, threshX = 60, threshY = 40):\n",
    "    for i in range(image.shape[0]):\n",
    "        \"\"\"if sum(image[i, :] == 0) >= threshX:\n",
    "            image[i, :] = 255\"\"\"\n",
    "        countBlack = 0\n",
    "        for j in range(image.shape[1]):\n",
    "            if image[i,j] == 0:\n",
    "                countBlack += 1\n",
    "            else:\n",
    "                if countBlack >= threshX:\n",
    "                    image[i,j-countBlack:j] = 255\n",
    "                countBlack = 0\n",
    "                \n",
    "    for i in range(image.shape[1]):\n",
    "        if sum(image[:,i] == 0) >= threshY:\n",
    "            image[:,i] = 255\n",
    "            \n",
    "        \"\"\"countBlack = 0\n",
    "        for j in range(image.shape[0]):\n",
    "            if image[j,i] == 0:\n",
    "                countBlack += 1\n",
    "            else:\n",
    "                if countBlack >= threshY:\n",
    "                    image[:,i] = 255\n",
    "                    print('Vertical line Deleted')\n",
    "                #countBlack = 0\"\"\"\n",
    "                \n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postRemoveProcessing(image):\n",
    "    image = cv2.medianBlur(image,3)\n",
    "    image = cv2.dilate(image, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)))\n",
    "    image = cv2.erode(image, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifiedRemoveBoxes(image, threshX = 50, threshY = 30):\n",
    "    for i in range(image.shape[0]):\n",
    "        countBlack = 0\n",
    "        for j in range(image.shape[1]):\n",
    "            if image[i,j] == 0:\n",
    "                countBlack += 1\n",
    "            else:\n",
    "                if countBlack >= threshX:\n",
    "                    image[i,j-countBlack:j] = 255\n",
    "                countBlack = 0\n",
    "                \n",
    "    for i in range(image.shape[1]):\n",
    "        countBlack = 0\n",
    "        for j in range(image.shape[0]):\n",
    "            if image[j,i] == 0:\n",
    "                countBlack += 1\n",
    "            else:\n",
    "                if countBlack >= threshY:\n",
    "                    image[j-countBlack:j,i] = 255\n",
    "                countBlack = 0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundaries(image, threshold=60):\n",
    "    boundaryRows = []\n",
    "    for i in range(image.shape[0]):\n",
    "        blackCount = 0\n",
    "        for j in range(image.shape[1]):\n",
    "            if image[i,j] == 0:\n",
    "                blackCount += 1\n",
    "            elif blackCount > threshold:\n",
    "                boundaryRows.append(i)\n",
    "                break\n",
    "            else:\n",
    "                blackCount = 0\n",
    "    print('Total Number of boundaries detected = ', len(boundaryRows))\n",
    "    return boundaryRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentCharacters(image, threshold=10):\n",
    "    image = removePadding2(image)\n",
    "    prev = 0\n",
    "    image[:,0] = 128\n",
    "    for i in range(1, image.shape[1]):\n",
    "        if (image[:,i] == 255).all() and (i-prev) > threshold:\n",
    "            image[:,i] = 128\n",
    "            prev = i\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizedImage = cv2.resize(image, None, fx=0.25, fy=0.25, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizedImage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayScaleImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3507, 2550)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayScaleImage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retVal, tempImage = cv2.threshold(grayScaleImage, 0, 255,  cv2.THRESH_BINARY + cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizedImage = cv2.medianBlur(tempImage,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines removed-\n",
      "Up = 10\n",
      "Left = 10\n",
      "Right = 179\n",
      "Down = 168\n"
     ]
    }
   ],
   "source": [
    "marginRemovedImage = removePadding(binarizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('./openCvImages/otsuBinarizedImage.jpg', binarizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('./openCvImages/marginRemovedImage.jpg', marginRemovedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of boundaries detected =  585\n"
     ]
    }
   ],
   "source": [
    "boundaries = getBoundaries(marginRemovedImage, threshold = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boundaries =  42\n"
     ]
    }
   ],
   "source": [
    "# assuming the boundaries of boxes are atleast 40 rows apart\n",
    "\n",
    "filteredBoundaries = [boundaries[0]]\n",
    "prev = boundaries[0]\n",
    "for val in boundaries[1:]:\n",
    "    if (val - prev) > 40:\n",
    "        prev = val\n",
    "        filteredBoundaries.append(val)\n",
    "print('Number of boundaries = ', len(filteredBoundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = marginRemovedImage.copy()\n",
    "temp[filteredBoundaries,:] = 128\n",
    "cv2.imwrite('./openCvImages/temp.jpg', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(filteredBoundaries)-1):\n",
    "    temp = marginRemovedImage[filteredBoundaries[i]:filteredBoundaries[i+1],:].copy()\n",
    "    temp = removeBoxes(temp, threshX=55, threshY=40)\n",
    "    processedImage = postRemoveProcessing(temp)\n",
    "    cv2.imwrite('./openCvImages/lines/{}.jpg'.format(i), processedImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold = 60 , number of lines detected = 650\n",
    "threshold = 100, number of lines detected = 628\n",
    "threshold = 200, number of lines detected = 585\n",
    "threshold = 500, number of lines detected = 466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1238dce48>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123ac42e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retVal, temp = cv2.threshold(cv2.cvtColor(cv2.imread('./openCvImages/lines/5.jpg'), cv2.COLOR_BGR2GRAY), 128, 255, cv2.THRESH_BINARY)\n",
    "cv2.medianBlur(temp,3)\n",
    "plt.figure(figsize=(10,200))\n",
    "plt.imshow(segmentCharacters(temp, threshold=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullFormImage = binarizedImage.copy()\n",
    "fullFormImage = modifiedRemoveBoxes(fullFormImage, threshX = 60, threshY=30)\n",
    "fullFormImage = cv2.medianBlur(fullFormImage,3)\n",
    "fullFormImage = cv2.dilate(fullFormImage, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)))\n",
    "fullFormImage = cv2.erode(fullFormImage, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)))\n",
    "cv2.imwrite('./openCvImages/processedImage.jpg', fullFormImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "formNumberImage = binarizedImage[330:450, 200:1000].copy()\n",
    "formNumberImage = removeBoxes(formNumberImage)\n",
    "formNumberImage = cv2.medianBlur(formNumberImage,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "formNumberImage = cv2.dilate(formNumberImage, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)))\n",
    "formNumberImage = cv2.erode(formNumberImage, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('originalImage',grayScaleImage[330:450, 200:1000])\n",
    "cv2.imshow('binarizedImage',binarizedImage[330:450, 200:1000])\n",
    "cv2.imshow('processedBinarizedImage',formNumberImage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function imshow in module matplotlib.pyplot:\n",
      "\n",
      "imshow(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, shape=None, filternorm=1, filterrad=4.0, imlim=None, resample=None, url=None, hold=None, data=None, **kwargs)\n",
      "    Display an image on the axes.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array_like, shape (n, m) or (n, m, 3) or (n, m, 4)\n",
      "        Display the image in `X` to current axes.  `X` may be an\n",
      "        array or a PIL image. If `X` is an array, it\n",
      "        can have the following shapes and types:\n",
      "    \n",
      "        - MxN -- values to be mapped (float or int)\n",
      "        - MxNx3 -- RGB (float or uint8)\n",
      "        - MxNx4 -- RGBA (float or uint8)\n",
      "    \n",
      "        The value for each component of MxNx3 and MxNx4 float arrays\n",
      "        should be in the range 0.0 to 1.0. MxN arrays are mapped\n",
      "        to colors based on the `norm` (mapping scalar to scalar)\n",
      "        and the `cmap` (mapping the normed scalar to a color).\n",
      "    \n",
      "    cmap : `~matplotlib.colors.Colormap`, optional, default: None\n",
      "        If None, default to rc `image.cmap` value. `cmap` is ignored\n",
      "        if `X` is 3-D, directly specifying RGB(A) values.\n",
      "    \n",
      "    aspect : ['auto' | 'equal' | scalar], optional, default: None\n",
      "        If 'auto', changes the image aspect ratio to match that of the\n",
      "        axes.\n",
      "    \n",
      "        If 'equal', and `extent` is None, changes the axes aspect ratio to\n",
      "        match that of the image. If `extent` is not `None`, the axes\n",
      "        aspect ratio is changed to match that of the extent.\n",
      "    \n",
      "        If None, default to rc ``image.aspect`` value.\n",
      "    \n",
      "    interpolation : string, optional, default: None\n",
      "        Acceptable values are 'none', 'nearest', 'bilinear', 'bicubic',\n",
      "        'spline16', 'spline36', 'hanning', 'hamming', 'hermite', 'kaiser',\n",
      "        'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell', 'sinc',\n",
      "        'lanczos'\n",
      "    \n",
      "        If `interpolation` is None, default to rc `image.interpolation`.\n",
      "        See also the `filternorm` and `filterrad` parameters.\n",
      "        If `interpolation` is 'none', then no interpolation is performed\n",
      "        on the Agg, ps and pdf backends. Other backends will fall back to\n",
      "        'nearest'.\n",
      "    \n",
      "    norm : `~matplotlib.colors.Normalize`, optional, default: None\n",
      "        A `~matplotlib.colors.Normalize` instance is used to scale\n",
      "        a 2-D float `X` input to the (0, 1) range for input to the\n",
      "        `cmap`. If `norm` is None, use the default func:`normalize`.\n",
      "        If `norm` is an instance of `~matplotlib.colors.NoNorm`,\n",
      "        `X` must be an array of integers that index directly into\n",
      "        the lookup table of the `cmap`.\n",
      "    \n",
      "    vmin, vmax : scalar, optional, default: None\n",
      "        `vmin` and `vmax` are used in conjunction with norm to normalize\n",
      "        luminance data.  Note if you pass a `norm` instance, your\n",
      "        settings for `vmin` and `vmax` will be ignored.\n",
      "    \n",
      "    alpha : scalar, optional, default: None\n",
      "        The alpha blending value, between 0 (transparent) and 1 (opaque)\n",
      "    \n",
      "    origin : ['upper' | 'lower'], optional, default: None\n",
      "        Place the [0,0] index of the array in the upper left or lower left\n",
      "        corner of the axes. If None, default to rc `image.origin`.\n",
      "    \n",
      "    extent : scalars (left, right, bottom, top), optional, default: None\n",
      "        The location, in data-coordinates, of the lower-left and\n",
      "        upper-right corners. If `None`, the image is positioned such that\n",
      "        the pixel centers fall on zero-based (row, column) indices.\n",
      "    \n",
      "    shape : scalars (columns, rows), optional, default: None\n",
      "        For raw buffer images\n",
      "    \n",
      "    filternorm : scalar, optional, default: 1\n",
      "        A parameter for the antigrain image resize filter.  From the\n",
      "        antigrain documentation, if `filternorm` = 1, the filter\n",
      "        normalizes integer values and corrects the rounding errors. It\n",
      "        doesn't do anything with the source floating point values, it\n",
      "        corrects only integers according to the rule of 1.0 which means\n",
      "        that any sum of pixel weights must be equal to 1.0.  So, the\n",
      "        filter function must produce a graph of the proper shape.\n",
      "    \n",
      "    filterrad : scalar, optional, default: 4.0\n",
      "        The filter radius for filters that have a radius parameter, i.e.\n",
      "        when interpolation is one of: 'sinc', 'lanczos' or 'blackman'\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    image : `~matplotlib.image.AxesImage`\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    **kwargs : `~matplotlib.artist.Artist` properties.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    matshow : Plot a matrix or an array as an image.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Unless *extent* is used, pixel centers will be located at integer\n",
      "    coordinates. In other words: the origin will coincide with the center\n",
      "    of pixel (0, 0).\n",
      "    \n",
      "    .. note::\n",
      "        In addition to the above described arguments, this function can take a\n",
      "        **data** keyword argument. If such a **data** argument is given, the\n",
      "        following arguments are replaced by **data[<arg>]**:\n",
      "    \n",
      "        * All positional and all keyword arguments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plt.imshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function destroyWindow:\n",
      "\n",
      "destroyWindow(...)\n",
      "    destroyWindow(winname) -> None\n",
      "    .   @brief Destroys the specified window.\n",
      "    .   \n",
      "    .   The function destroyWindow destroys the window with the given name.\n",
      "    .   \n",
      "    .   @param winname Name of the window to be destroyed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.destroyWindow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function threshold:\n",
      "\n",
      "threshold(...)\n",
      "    threshold(src, thresh, maxval, type[, dst]) -> retval, dst\n",
      "    .   @brief Applies a fixed-level threshold to each array element.\n",
      "    .   \n",
      "    .   The function applies fixed-level thresholding to a multiple-channel array. The function is typically\n",
      "    .   used to get a bi-level (binary) image out of a grayscale image ( #compare could be also used for\n",
      "    .   this purpose) or for removing a noise, that is, filtering out pixels with too small or too large\n",
      "    .   values. There are several types of thresholding supported by the function. They are determined by\n",
      "    .   type parameter.\n",
      "    .   \n",
      "    .   Also, the special values #THRESH_OTSU or #THRESH_TRIANGLE may be combined with one of the\n",
      "    .   above values. In these cases, the function determines the optimal threshold value using the Otsu's\n",
      "    .   or Triangle algorithm and uses it instead of the specified thresh.\n",
      "    .   \n",
      "    .   @note Currently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.\n",
      "    .   \n",
      "    .   @param src input array (multiple-channel, 8-bit or 32-bit floating point).\n",
      "    .   @param dst output array of the same size  and type and the same number of channels as src.\n",
      "    .   @param thresh threshold value.\n",
      "    .   @param maxval maximum value to use with the #THRESH_BINARY and #THRESH_BINARY_INV thresholding\n",
      "    .   types.\n",
      "    .   @param type thresholding type (see #ThresholdTypes).\n",
      "    .   @return the computed threshold value if Otsu's or Triangle methods used.\n",
      "    .   \n",
      "    .   @sa  adaptiveThreshold, findContours, compare, min, max\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function GaussianBlur:\n",
      "\n",
      "GaussianBlur(...)\n",
      "    GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]]) -> dst\n",
      "    .   @brief Blurs an image using a Gaussian filter.\n",
      "    .   \n",
      "    .   The function convolves the source image with the specified Gaussian kernel. In-place filtering is\n",
      "    .   supported.\n",
      "    .   \n",
      "    .   @param src input image; the image can have any number of channels, which are processed\n",
      "    .   independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "    .   @param dst output image of the same size and type as src.\n",
      "    .   @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be\n",
      "    .   positive and odd. Or, they can be zero's and then they are computed from sigma.\n",
      "    .   @param sigmaX Gaussian kernel standard deviation in X direction.\n",
      "    .   @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be\n",
      "    .   equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,\n",
      "    .   respectively (see #getGaussianKernel for details); to fully control the result regardless of\n",
      "    .   possible future modifications of all this semantics, it is recommended to specify all of ksize,\n",
      "    .   sigmaX, and sigmaY.\n",
      "    .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "    .   \n",
      "    .   @sa  sepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.GaussianBlur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
