{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotROC(fpr, tpr):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve, area = {:.2}'.format(auc(fpr, tpr)))\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numpy.loadtxt('modifiedTraining.csv', delimiter=',')\n",
    "labels = numpy.loadtxt('labels.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15420, 159)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15420,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(features, labels, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingX, validX, trainingY, validY = train_test_split(trainX, trainY, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SMOTE for over-sampling\n",
    "sm = SMOTE(kind='regular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sm.fit_sample(trainingX, trainingY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fraudulent cases = 8147\n",
      "Number of true cases = 8147\n"
     ]
    }
   ],
   "source": [
    "print('Number of fraudulent cases = {}\\nNumber of true cases = {}'.format(sum(y[:] == 1), sum(y[:] == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing training data to zero mean and unit standard deviation\n",
    "x -= numpy.mean(x, axis=0)\n",
    "sd = numpy.std(x, axis=0)\n",
    "sd[sd[:] == 0] = 1\n",
    "x /= sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe1 = OneHotEncoder()\n",
    "ohe1.fit(y.reshape(-1,1))\n",
    "transformedTrainingY = ohe1.transform(y.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX -= numpy.mean(testX, axis=0)\n",
    "sd = numpy.std(testX, axis=0)\n",
    "sd[sd[:] == 0] = 1\n",
    "testX /= sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "validX -= numpy.mean(validX, axis=0)\n",
    "sd = numpy.std(validX, axis=0)\n",
    "sd[sd[:] == 0] = 1\n",
    "validX /= sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "callBack = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16294 samples, validate on 2892 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.5766 - acc: 0.6907 - val_loss: 0.7016 - val_acc: 0.5899\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4343 - acc: 0.8008 - val_loss: 0.5850 - val_acc: 0.6680\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3663 - acc: 0.8423 - val_loss: 0.6017 - val_acc: 0.6743\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3131 - acc: 0.8743 - val_loss: 0.5448 - val_acc: 0.7089\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.2662 - acc: 0.9022 - val_loss: 0.5562 - val_acc: 0.7133\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.2244 - acc: 0.9218 - val_loss: 0.5251 - val_acc: 0.7459\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.1894 - acc: 0.9380 - val_loss: 0.4906 - val_acc: 0.7711\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.1599 - acc: 0.9502 - val_loss: 0.5482 - val_acc: 0.7524\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.1336 - acc: 0.9606 - val_loss: 0.4472 - val_acc: 0.8074\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.1134 - acc: 0.9696 - val_loss: 0.4791 - val_acc: 0.8015\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.0957 - acc: 0.9756 - val_loss: 0.4833 - val_acc: 0.8043\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.0816 - acc: 0.9795 - val_loss: 0.5239 - val_acc: 0.7991\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.0697 - acc: 0.9834 - val_loss: 0.4918 - val_acc: 0.8174\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.0593 - acc: 0.9867 - val_loss: 0.5101 - val_acc: 0.8188\n",
      "Test Loss = 0.4919982980661355 , Test Accuracy = 0.821011673198136, AUC = 0.7612150310685765\n",
      "[[3078  573]\n",
      " [ 117   87]]\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.001, momentum=0.9)\n",
    "set_random_seed(1)\n",
    "numpy.random.seed(1)\n",
    "model = Sequential()\n",
    "model.add(Dense(60, activation='relu', input_shape=(159,)))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "model.fit(x, transformedTrainingY, batch_size = 32, epochs=50, verbose=2, validation_data=(validX,ohe1.transform(validY.reshape(-1,1)).toarray()), callbacks=[callBack])\n",
    "loss, accuracy = model.evaluate(testX, ohe1.transform(testY.reshape(-1,1)).toarray(), verbose=2)\n",
    "predicted = model.predict(testX)\n",
    "print('Test Loss = {} , Test Accuracy = {}, AUC = {}'.format(loss, accuracy, roc_auc_score(testY, predicted[:,1])))\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix(testY, numpy.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(testY, predicted[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, (0.0, 1.999993))\n",
      "(0.0002738975623116954, (0.0, 0.99999297))\n",
      "(0.0010955902492467817, (0.0, 0.9998317))\n",
      "(0.0010955902492467817, (0.00980392156862745, 0.99974364))\n",
      "(0.001917282936181868, (0.00980392156862745, 0.9995486))\n",
      "(0.001917282936181868, (0.0196078431372549, 0.99926466))\n",
      "(0.0024650780608052587, (0.0196078431372549, 0.9991696))\n",
      "(0.0024650780608052587, (0.024509803921568627, 0.9991217))\n",
      "(0.002738975623116954, (0.024509803921568627, 0.9990012))\n",
      "(0.002738975623116954, (0.029411764705882353, 0.9989729))\n",
      "(0.0046562585592988225, (0.029411764705882353, 0.9984145))\n",
      "(0.0046562585592988225, (0.03431372549019608, 0.9982632))\n",
      "(0.006025746370857299, (0.03431372549019608, 0.9974849))\n",
      "(0.006025746370857299, (0.0392156862745098, 0.9974045))\n",
      "(0.007395234182415777, (0.0392156862745098, 0.99644655))\n",
      "(0.007395234182415777, (0.04411764705882353, 0.99618715))\n",
      "(0.008764721993974253, (0.04411764705882353, 0.99569345))\n",
      "(0.008764721993974253, (0.049019607843137254, 0.99563676))\n",
      "(0.010682004930156122, (0.049019607843137254, 0.99445397))\n",
      "(0.010682004930156122, (0.05392156862745098, 0.9944417))\n",
      "(0.011229800054779512, (0.05392156862745098, 0.9934546))\n",
      "(0.011229800054779512, (0.058823529411764705, 0.99320203))\n",
      "(0.011777595179402904, (0.058823529411764705, 0.9924064))\n",
      "(0.011777595179402904, (0.06862745098039216, 0.9920948))\n",
      "(0.012873185428649686, (0.06862745098039216, 0.9910665))\n",
      "(0.012873185428649686, (0.0784313725490196, 0.9909752))\n",
      "(0.013968775677896467, (0.0784313725490196, 0.99014133))\n",
      "(0.013968775677896467, (0.08823529411764706, 0.989907))\n",
      "(0.015064365927143249, (0.08823529411764706, 0.98748857))\n",
      "(0.015064365927143249, (0.09313725490196079, 0.9874557))\n",
      "(0.015612161051766639, (0.09313725490196079, 0.98655593))\n",
      "(0.015612161051766639, (0.09803921568627451, 0.9865469))\n",
      "(0.020542317173377157, (0.09803921568627451, 0.9833377))\n",
      "(0.020542317173377157, (0.10294117647058823, 0.98279417))\n",
      "(0.02273349767187072, (0.10294117647058823, 0.9784414))\n",
      "(0.02273349767187072, (0.10784313725490197, 0.978215))\n",
      "(0.0238290879211175, (0.10784313725490197, 0.9774361))\n",
      "(0.0238290879211175, (0.11274509803921569, 0.9772215))\n",
      "(0.02684196110654615, (0.11274509803921569, 0.9738957))\n",
      "(0.02684196110654615, (0.11764705882352941, 0.9732195))\n",
      "(0.02766365379348124, (0.11764705882352941, 0.97248894))\n",
      "(0.02766365379348124, (0.12254901960784313, 0.97246504))\n",
      "(0.02821144891810463, (0.12254901960784313, 0.97225696))\n",
      "(0.02821144891810463, (0.12745098039215685, 0.9712152))\n",
      "(0.030402629416598194, (0.12745098039215685, 0.9670137))\n",
      "(0.030402629416598194, (0.14215686274509803, 0.9650541))\n",
      "(0.030676526978909886, (0.14215686274509803, 0.964303))\n",
      "(0.030676526978909886, (0.14705882352941177, 0.9642127))\n",
      "(0.03396329772665023, (0.14705882352941177, 0.9575956))\n",
      "(0.03396329772665023, (0.15196078431372548, 0.9569724))\n",
      "(0.03451109285127362, (0.15196078431372548, 0.9566794))\n",
      "(0.03451109285127362, (0.1568627450980392, 0.95565796))\n",
      "(0.038071761161325664, (0.1568627450980392, 0.95024544))\n",
      "(0.038071761161325664, (0.16176470588235295, 0.9495225))\n",
      "(0.03834565872363736, (0.16176470588235295, 0.9492858))\n",
      "(0.03834565872363736, (0.16666666666666666, 0.94896054))\n",
      "(0.03889345384826075, (0.16666666666666666, 0.9473227))\n",
      "(0.03889345384826075, (0.1715686274509804, 0.9454764))\n",
      "(0.0421802245960011, (0.1715686274509804, 0.93927336))\n",
      "(0.0421802245960011, (0.17647058823529413, 0.93869823))\n",
      "(0.043275814845247874, (0.17647058823529413, 0.9360787))\n",
      "(0.043275814845247874, (0.18137254901960784, 0.9357228))\n",
      "(0.047110380717611616, (0.18137254901960784, 0.92192435))\n",
      "(0.047110380717611616, (0.18627450980392157, 0.92134446))\n",
      "(0.0479320734045467, (0.18627450980392157, 0.91947895))\n",
      "(0.0479320734045467, (0.19117647058823528, 0.91872734))\n",
      "(0.04820597096685839, (0.19117647058823528, 0.91861624))\n",
      "(0.04820597096685839, (0.20098039215686275, 0.9173509))\n",
      "(0.05204053683922213, (0.20098039215686275, 0.9088306))\n",
      "(0.05204053683922213, (0.20588235294117646, 0.9080866))\n",
      "(0.05286222952615722, (0.20588235294117646, 0.90758336))\n",
      "(0.05286222952615722, (0.21568627450980393, 0.90456194))\n",
      "(0.05587510271158587, (0.21568627450980393, 0.89814955))\n",
      "(0.05587510271158587, (0.22058823529411764, 0.8975663))\n",
      "(0.057244590523144345, (0.22058823529411764, 0.8924717))\n",
      "(0.057244590523144345, (0.22549019607843138, 0.8921072))\n",
      "(0.06053136127088469, (0.22549019607843138, 0.8840477))\n",
      "(0.06053136127088469, (0.23039215686274508, 0.8820554))\n",
      "(0.06463982470556012, (0.23039215686274508, 0.8664075))\n",
      "(0.06463982470556012, (0.23529411764705882, 0.8661471))\n",
      "(0.0660093125171186, (0.23529411764705882, 0.8628185))\n",
      "(0.0660093125171186, (0.24019607843137256, 0.8625834))\n",
      "(0.06874828814023555, (0.24019607843137256, 0.84949994))\n",
      "(0.06874828814023555, (0.24509803921568626, 0.8492794))\n",
      "(0.06902218570254724, (0.24509803921568626, 0.84918433))\n",
      "(0.06902218570254724, (0.25, 0.8483378))\n",
      "(0.07011777595179403, (0.25, 0.84697074))\n",
      "(0.07011777595179403, (0.2549019607843137, 0.8459699))\n",
      "(0.07258285401259928, (0.2549019607843137, 0.8326099))\n",
      "(0.07258285401259928, (0.25980392156862747, 0.8324406))\n",
      "(0.07943029307039168, (0.25980392156862747, 0.8134235))\n",
      "(0.07943029307039168, (0.2647058823529412, 0.812167))\n",
      "(0.08134757600657354, (0.2647058823529412, 0.80856997))\n",
      "(0.08134757600657354, (0.2696078431372549, 0.8073856))\n",
      "(0.08408655162969049, (0.2696078431372549, 0.80085045))\n",
      "(0.08408655162969049, (0.27450980392156865, 0.80031264))\n",
      "(0.08901670775130101, (0.27450980392156865, 0.7863157))\n",
      "(0.08901670775130101, (0.28431372549019607, 0.7849342))\n",
      "(0.09422076143522323, (0.28431372549019607, 0.76775664))\n",
      "(0.09422076143522323, (0.28921568627450983, 0.7662574))\n",
      "(0.09449465899753492, (0.28921568627450983, 0.76381177))\n",
      "(0.09449465899753492, (0.29411764705882354, 0.7625608))\n",
      "(0.09531635168447, (0.29411764705882354, 0.75973266))\n",
      "(0.09531635168447, (0.29901960784313725, 0.7593852))\n",
      "(0.0958641468090934, (0.29901960784313725, 0.75747854))\n",
      "(0.0958641468090934, (0.30392156862745096, 0.75695235))\n",
      "(0.09668583949602849, (0.30392156862745096, 0.75267714))\n",
      "(0.09668583949602849, (0.3088235294117647, 0.75138056))\n",
      "(0.09915091755683375, (0.3088235294117647, 0.7381825))\n",
      "(0.09915091755683375, (0.3137254901960784, 0.7379228))\n",
      "(0.09969871268145714, (0.3137254901960784, 0.7358647))\n",
      "(0.09969871268145714, (0.3235294117647059, 0.7349618))\n",
      "(0.09997261024376883, (0.3235294117647059, 0.734683))\n",
      "(0.09997261024376883, (0.3284313725490196, 0.73433614))\n",
      "(0.10106820049301561, (0.3284313725490196, 0.7278884))\n",
      "(0.10106820049301561, (0.3333333333333333, 0.72402024))\n",
      "(0.11120241029854834, (0.3333333333333333, 0.6876071))\n",
      "(0.11120241029854834, (0.3382352941176471, 0.6868119))\n",
      "(0.11284579567241852, (0.3382352941176471, 0.67605716))\n",
      "(0.11284579567241852, (0.3431372549019608, 0.6756232))\n",
      "(0.11394138592166529, (0.3431372549019608, 0.6710093))\n",
      "(0.11394138592166529, (0.3480392156862745, 0.6690523))\n",
      "(0.11531087373322377, (0.3480392156862745, 0.66366273))\n",
      "(0.11531087373322377, (0.35294117647058826, 0.66280943))\n",
      "(0.11640646398247055, (0.35294117647058826, 0.6560233))\n",
      "(0.11640646398247055, (0.35784313725490197, 0.65320003))\n",
      "(0.11722815666940564, (0.35784313725490197, 0.6521809))\n",
      "(0.11722815666940564, (0.3627450980392157, 0.6506046))\n",
      "(0.11750205423171733, (0.3627450980392157, 0.6503213))\n",
      "(0.11750205423171733, (0.36764705882352944, 0.6503049))\n",
      "(0.11887154204327581, (0.36764705882352944, 0.6475806))\n",
      "(0.11887154204327581, (0.37254901960784315, 0.6455761))\n",
      "(0.12051492741714599, (0.37254901960784315, 0.635513))\n",
      "(0.12051492741714599, (0.37745098039215685, 0.6350424))\n",
      "(0.12161051766639278, (0.37745098039215685, 0.63107294))\n",
      "(0.12161051766639278, (0.38235294117647056, 0.6293233))\n",
      "(0.12352780060257464, (0.38235294117647056, 0.62539846))\n",
      "(0.12352780060257464, (0.3872549019607843, 0.6251194))\n",
      "(0.12489728841413311, (0.3872549019607843, 0.62152016))\n",
      "(0.12489728841413311, (0.39705882352941174, 0.6189074))\n",
      "(0.12873185428649686, (0.39705882352941174, 0.61177516))\n",
      "(0.12873185428649686, (0.4019607843137255, 0.6113674))\n",
      "(0.12927964941112025, (0.4019607843137255, 0.6085269))\n",
      "(0.12927964941112025, (0.4068627450980392, 0.60826796))\n",
      "(0.13311421528348397, (0.4068627450980392, 0.59265786))\n",
      "(0.13311421528348397, (0.4117647058823529, 0.5901988))\n",
      "(0.13804437140509448, (0.4117647058823529, 0.562024))\n",
      "(0.13804437140509448, (0.4166666666666667, 0.56193155))\n",
      "(0.1500958641468091, (0.4166666666666667, 0.5301489))\n",
      "(0.1500958641468091, (0.4215686274509804, 0.5301237))\n",
      "(0.1552999178307313, (0.4215686274509804, 0.5089219))\n",
      "(0.1552999178307313, (0.4264705882352941, 0.50891733))\n",
      "(0.16516023007395234, (0.4264705882352941, 0.46968728))\n",
      "(0.16516023007395234, (0.43137254901960786, 0.4656862))\n",
      "(0.17063818132018624, (0.43137254901960786, 0.45170745))\n",
      "(0.17063818132018624, (0.4362745098039216, 0.4516555))\n",
      "(0.17228156669405642, (0.4362745098039216, 0.44734976))\n",
      "(0.17228156669405642, (0.4411764705882353, 0.44730103))\n",
      "(0.17447274719255, (0.4411764705882353, 0.44447815))\n",
      "(0.17447274719255, (0.44607843137254904, 0.44387662))\n",
      "(0.17639003012873186, (0.44607843137254904, 0.43951413))\n",
      "(0.17639003012873186, (0.45098039215686275, 0.43840495))\n",
      "(0.1810462886880307, (0.45098039215686275, 0.4242216))\n",
      "(0.1810462886880307, (0.45588235294117646, 0.42338735))\n",
      "(0.18268967406190084, (0.45588235294117646, 0.41322696))\n",
      "(0.18268967406190084, (0.46078431372549017, 0.4113404))\n",
      "(0.18378526431114764, (0.46078431372549017, 0.4102821))\n",
      "(0.18378526431114764, (0.46568627450980393, 0.40970278))\n",
      "(0.19008490824431662, (0.46568627450980393, 0.3880393))\n",
      "(0.19008490824431662, (0.47549019607843135, 0.38582757))\n",
      "(0.19446726924130375, (0.47549019607843135, 0.36943647))\n",
      "(0.19446726924130375, (0.4803921568627451, 0.369375))\n",
      "(0.19501506436592714, (0.4803921568627451, 0.36897323))\n",
      "(0.19501506436592714, (0.4852941176470588, 0.3684065))\n",
      "(0.1988496302382909, (0.4852941176470588, 0.35453284))\n",
      "(0.1988496302382909, (0.4950980392156863, 0.3531423))\n",
      "(0.2035058887975897, (0.4950980392156863, 0.33606392))\n",
      "(0.2035058887975897, (0.5, 0.33524394))\n",
      "(0.20514927417145987, (0.5, 0.32718986))\n",
      "(0.20514927417145987, (0.5147058823529411, 0.32471395))\n",
      "(0.2152834839769926, (0.5147058823529411, 0.28993654))\n",
      "(0.2152834839769926, (0.5196078431372549, 0.2876353))\n",
      "(0.2155573815393043, (0.5196078431372549, 0.28747863))\n",
      "(0.2155573815393043, (0.5294117647058824, 0.28676638))\n",
      "(0.21857025472473296, (0.5294117647058824, 0.27883908))\n",
      "(0.21857025472473296, (0.5343137254901961, 0.27865165))\n",
      "(0.21939194741166804, (0.5343137254901961, 0.2708909))\n",
      "(0.21939194741166804, (0.5392156862745098, 0.2686402))\n",
      "(0.22295261572172007, (0.5392156862745098, 0.26048553))\n",
      "(0.22295261572172007, (0.5441176470588235, 0.25917092))\n",
      "(0.23062174746644754, (0.5441176470588235, 0.24168488))\n",
      "(0.23062174746644754, (0.5490196078431373, 0.24099581))\n",
      "(0.2341824157764996, (0.5490196078431373, 0.22959329))\n",
      "(0.2341824157764996, (0.553921568627451, 0.22904778))\n",
      "(0.23555190358805805, (0.553921568627451, 0.22255145))\n",
      "(0.23555190358805805, (0.5588235294117647, 0.22127399))\n",
      "(0.24760339632977266, (0.5588235294117647, 0.19382052))\n",
      "(0.24760339632977266, (0.5637254901960784, 0.19349182))\n",
      "(0.25417693782525336, (0.5637254901960784, 0.17774588))\n",
      "(0.25417693782525336, (0.5686274509803921, 0.17768134))\n",
      "(0.25554642563681185, (0.5686274509803921, 0.1766146))\n",
      "(0.25554642563681185, (0.5735294117647058, 0.17647752))\n",
      "(0.2558203231991235, (0.5735294117647058, 0.1759364))\n",
      "(0.2558203231991235, (0.5784313725490197, 0.17527902))\n",
      "(0.25691591344837034, (0.5784313725490197, 0.17268707))\n",
      "(0.25691591344837034, (0.5833333333333334, 0.17265253))\n",
      "(0.26157217200766913, (0.5833333333333334, 0.16598195))\n",
      "(0.26157217200766913, (0.5882352941176471, 0.1652566))\n",
      "(0.26184606956998085, (0.5882352941176471, 0.1642582))\n",
      "(0.26184606956998085, (0.5931372549019608, 0.16251008))\n",
      "(0.26431114763078606, (0.5931372549019608, 0.16031523))\n",
      "(0.26431114763078606, (0.5980392156862745, 0.15846947))\n",
      "(0.27061079156395507, (0.5980392156862745, 0.1490142))\n",
      "(0.27061079156395507, (0.6029411764705882, 0.14767456))\n",
      "(0.27800602574637084, (0.6029411764705882, 0.13730647))\n",
      "(0.27800602574637084, (0.6078431372549019, 0.13638774))\n",
      "(0.27855382087099423, (0.6078431372549019, 0.13601989))\n",
      "(0.27855382087099423, (0.6127450980392157, 0.13598964))\n",
      "(0.2812927964941112, (0.6127450980392157, 0.13186108))\n",
      "(0.2812927964941112, (0.6176470588235294, 0.13176359))\n",
      "(0.28512736236647496, (0.6176470588235294, 0.12526093))\n",
      "(0.28512736236647496, (0.6225490196078431, 0.1249053))\n",
      "(0.2881402355519036, (0.6225490196078431, 0.118291885))\n",
      "(0.2881402355519036, (0.6274509803921569, 0.117992274))\n",
      "(0.28923582580115037, (0.6274509803921569, 0.116333164))\n",
      "(0.28923582580115037, (0.6323529411764706, 0.116179))\n",
      "(0.30347849904135854, (0.6323529411764706, 0.09586253))\n",
      "(0.30347849904135854, (0.6372549019607843, 0.09500807))\n",
      "(0.30703916735141057, (0.6372549019607843, 0.09104202))\n",
      "(0.30703916735141057, (0.6470588235294118, 0.09090411))\n",
      "(0.30813475760065734, (0.6470588235294118, 0.08957593))\n",
      "(0.30813475760065734, (0.6519607843137255, 0.089443296))\n",
      "(0.3086825527252807, (0.6519607843137255, 0.08930527))\n",
      "(0.3086825527252807, (0.6568627450980392, 0.088909425))\n",
      "(0.30977814297452755, (0.6568627450980392, 0.086779065))\n",
      "(0.30977814297452755, (0.6666666666666666, 0.08565063))\n",
      "(0.3188167625308135, (0.6666666666666666, 0.07554402))\n",
      "(0.3188167625308135, (0.6715686274509803, 0.075209916))\n",
      "(0.31991235278006025, (0.6715686274509803, 0.074314564))\n",
      "(0.31991235278006025, (0.6764705882352942, 0.07418701))\n",
      "(0.321007943029307, (0.6764705882352942, 0.073174484))\n",
      "(0.321007943029307, (0.6813725490196079, 0.072940975))\n",
      "(0.32539030402629415, (0.6813725490196079, 0.06868573))\n",
      "(0.32539030402629415, (0.6862745098039216, 0.06857017))\n",
      "(0.32812927964941113, (0.6862745098039216, 0.06584611))\n",
      "(0.32812927964941113, (0.696078431372549, 0.06554853))\n",
      "(0.3286770747740345, (0.696078431372549, 0.064895436))\n",
      "(0.3286770747740345, (0.7009803921568627, 0.06488699))\n",
      "(0.33360723089564503, (0.7009803921568627, 0.062724285))\n",
      "(0.33360723089564503, (0.7058823529411765, 0.061631978))\n",
      "(0.33634620651876196, (0.7058823529411765, 0.05918195))\n",
      "(0.33634620651876196, (0.7107843137254902, 0.05900214))\n",
      "(0.33716789920569706, (0.7107843137254902, 0.058550946))\n",
      "(0.33716789920569706, (0.7156862745098039, 0.058402114))\n",
      "(0.33798959189263217, (0.7156862745098039, 0.05787301))\n",
      "(0.33798959189263217, (0.7205882352941176, 0.057267092))\n",
      "(0.33990687482881404, (0.7205882352941176, 0.05575286))\n",
      "(0.33990687482881404, (0.7254901960784313, 0.055749122))\n",
      "(0.3442892358258012, (0.7254901960784313, 0.05382749))\n",
      "(0.3442892358258012, (0.7303921568627451, 0.053804837))\n",
      "(0.3451109285127362, (0.7303921568627451, 0.052471273))\n",
      "(0.3451109285127362, (0.7401960784313726, 0.05229183))\n",
      "(0.34593262119967133, (0.7401960784313726, 0.05194938))\n",
      "(0.34593262119967133, (0.7450980392156863, 0.051923506))\n",
      "(0.3489454943851, (0.7450980392156863, 0.050054006))\n",
      "(0.3489454943851, (0.7549019607843137, 0.049761426))\n",
      "(0.36510545056149, (0.7549019607843137, 0.04152996))\n",
      "(0.36510545056149, (0.7598039215686274, 0.04146011))\n",
      "(0.3763352506162695, (0.7598039215686274, 0.03589251))\n",
      "(0.3763352506162695, (0.7647058823529411, 0.035848495))\n",
      "(0.3774308408655163, (0.7647058823529411, 0.035397))\n",
      "(0.3774308408655163, (0.7696078431372549, 0.03528265))\n",
      "(0.38099150917556834, (0.7696078431372549, 0.033940893))\n",
      "(0.38099150917556834, (0.7745098039215687, 0.033710707))\n",
      "(0.38153930430019173, (0.7745098039215687, 0.033345606))\n",
      "(0.38153930430019173, (0.7794117647058824, 0.033304602))\n",
      "(0.384004382360997, (0.7794117647058824, 0.032342296))\n",
      "(0.384004382360997, (0.7843137254901961, 0.03215248))\n",
      "(0.38729115310873735, (0.7843137254901961, 0.031022754))\n",
      "(0.38729115310873735, (0.7892156862745098, 0.031004172))\n",
      "(0.3900301287318543, (0.7892156862745098, 0.029988794))\n",
      "(0.3900301287318543, (0.7941176470588235, 0.029983401))\n",
      "(0.39222130923034787, (0.7941176470588235, 0.029322056))\n",
      "(0.39222130923034787, (0.7990196078431373, 0.029321797))\n",
      "(0.39605587510271156, (0.7990196078431373, 0.027851563))\n",
      "(0.39605587510271156, (0.803921568627451, 0.027706966))\n",
      "(0.4009860312243221, (0.803921568627451, 0.026112389))\n",
      "(0.4009860312243221, (0.8088235294117647, 0.025974976))\n",
      "(0.4020816214735689, (0.8088235294117647, 0.025724726))\n",
      "(0.4020816214735689, (0.8137254901960784, 0.025554134))\n",
      "(0.4026294165981923, (0.8137254901960784, 0.025235394))\n",
      "(0.4026294165981923, (0.8186274509803921, 0.025155367))\n",
      "(0.4053683922213092, (0.8186274509803921, 0.023988133))\n",
      "(0.4053683922213092, (0.8235294117647058, 0.023959948))\n",
      "(0.4196110654615174, (0.8235294117647058, 0.019980505))\n",
      "(0.4196110654615174, (0.8284313725490197, 0.019781062))\n",
      "(0.4245412215831279, (0.8284313725490197, 0.01875409))\n",
      "(0.4245412215831279, (0.8333333333333334, 0.018739352))\n",
      "(0.42810188989317993, (0.8333333333333334, 0.018219419))\n",
      "(0.42810188989317993, (0.8382352941176471, 0.018218271))\n",
      "(0.4319364557655437, (0.8382352941176471, 0.016556058))\n",
      "(0.4319364557655437, (0.8431372549019608, 0.016519405))\n",
      "(0.43385373870172556, (0.8431372549019608, 0.016232526))\n",
      "(0.43385373870172556, (0.8480392156862745, 0.01611071))\n",
      "(0.4480964119419337, (0.8480392156862745, 0.013273269))\n",
      "(0.4480964119419337, (0.8529411764705882, 0.013235862))\n",
      "(0.4554916461243495, (0.8529411764705882, 0.0119021945))\n",
      "(0.4554916461243495, (0.8578431372549019, 0.011816383))\n",
      "(0.4615173924952068, (0.8578431372549019, 0.010775283))\n",
      "(0.4615173924952068, (0.8627450980392157, 0.010735832))\n",
      "(0.4664475486168173, (0.8627450980392157, 0.010199268))\n",
      "(0.4664475486168173, (0.8676470588235294, 0.01018537))\n",
      "(0.47356888523692137, (0.8676470588235294, 0.009093768))\n",
      "(0.47356888523692137, (0.8725490196078431, 0.009004786))\n",
      "(0.4746644754861682, (0.8725490196078431, 0.008887545))\n",
      "(0.4746644754861682, (0.8774509803921569, 0.008853897))\n",
      "(0.4782251437962202, (0.8774509803921569, 0.0085075535))\n",
      "(0.4782251437962202, (0.8823529411764706, 0.008479613))\n",
      "(0.48205970966858397, (0.8823529411764706, 0.0077652303))\n",
      "(0.48205970966858397, (0.8872549019607843, 0.007715744))\n",
      "(0.4897288414133114, (0.8872549019607843, 0.0069524087))\n",
      "(0.4897288414133114, (0.8921568627450981, 0.0069105173))\n",
      "(0.5026020268419611, (0.8921568627450981, 0.0058282153))\n",
      "(0.5026020268419611, (0.8970588235294118, 0.0058019157))\n",
      "(0.5047932073404546, (0.8970588235294118, 0.005631149))\n",
      "(0.5047932073404546, (0.9019607843137255, 0.005630348))\n",
      "(0.50725828540126, (0.9019607843137255, 0.0054851654))\n",
      "(0.50725828540126, (0.9068627450980392, 0.005465557))\n",
      "(0.5154752122706108, (0.9068627450980392, 0.0047366684))\n",
      "(0.5154752122706108, (0.9117647058823529, 0.0047278013))\n",
      "(0.5193097781429745, (0.9117647058823529, 0.0044261524))\n",
      "(0.5193097781429745, (0.9166666666666666, 0.0044233943))\n",
      "(0.5267050123253904, (0.9166666666666666, 0.0039746305))\n",
      "(0.5267050123253904, (0.9215686274509803, 0.0039142435))\n",
      "(0.5384826075047932, (0.9215686274509803, 0.0032012095))\n",
      "(0.5384826075047932, (0.9264705882352942, 0.003198633))\n",
      "(0.551355792933443, (0.9264705882352942, 0.0025734517))\n",
      "(0.551355792933443, (0.9313725490196079, 0.002572835))\n",
      "(0.5519035880580663, (0.9313725490196079, 0.0025490967))\n",
      "(0.5519035880580663, (0.9362745098039216, 0.0025269))\n",
      "(0.5532730758696247, (0.9362745098039216, 0.0024561875))\n",
      "(0.5532730758696247, (0.9411764705882353, 0.0024539558))\n",
      "(0.5551903588058066, (0.9411764705882353, 0.00236521))\n",
      "(0.5551903588058066, (0.946078431372549, 0.002351391))\n",
      "(0.5631333881128457, (0.946078431372549, 0.0020506324))\n",
      "(0.5631333881128457, (0.9509803921568627, 0.0020483385))\n",
      "(0.5708025198575732, (0.9509803921568627, 0.001772772))\n",
      "(0.5708025198575732, (0.9558823529411765, 0.0017597685))\n",
      "(0.57189811010682, (0.9558823529411765, 0.001701828))\n",
      "(0.57189811010682, (0.9607843137254902, 0.001699337))\n",
      "(0.6373596274993153, (0.9607843137254902, 0.0006894203))\n",
      "(0.6373596274993153, (0.9656862745098039, 0.0006863215))\n",
      "(0.6628321007943029, (0.9656862745098039, 0.00045025567))\n",
      "(0.6628321007943029, (0.9705882352941176, 0.00044757838))\n",
      "(0.6814571350314982, (0.9705882352941176, 0.00029524157))\n",
      "(0.6814571350314982, (0.9754901960784313, 0.00029459712))\n",
      "(0.7608874281018899, (0.9754901960784313, 7.0005626e-05))\n",
      "(0.7608874281018899, (0.9803921568627451, 6.981102e-05))\n",
      "(0.8414133114215283, (0.9803921568627451, 1.2412135e-05))\n",
      "(0.8414133114215283, (0.9852941176470589, 1.22821875e-05))\n",
      "(0.8592166529717885, (0.9852941176470589, 8.257409e-06))\n",
      "(0.8592166529717885, (0.9901960784313726, 8.216499e-06))\n",
      "(0.9186524239934265, (0.9901960784313726, 1.3833288e-06))\n",
      "(0.9186524239934265, (0.9950980392156863, 1.3665583e-06))\n",
      "(0.9764448096411942, (0.9950980392156863, 3.9212143e-08))\n",
      "(0.9764448096411942, (1.0, 3.7974846e-08))\n",
      "(1.0, (1.0, 1.0466587e-26))\n"
     ]
    }
   ],
   "source": [
    "for val in list(zip(fpr, list(zip(tpr, threshold)))):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41330ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotROC(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = numpy.zeros(testY.shape)\n",
    "p[predicted[:,1] > 0.3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPredicted = numpy.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2879,  772],\n",
       "       [  99,  105]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(testY, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11972633979475485"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(testY, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5147058823529411"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(testY, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1942645698427382"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(testY, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7740596627756161"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(testY, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module keras.models:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs) method of keras.models.Sequential instance\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    # Arguments\n",
      "        x: Numpy array of training data.\n",
      "            If the input layer in the model is named, you can also pass a\n",
      "            dictionary mapping the input name to a Numpy array.\n",
      "            `x` can be `None` (default) if feeding from\n",
      "            framework-native tensors (e.g. TensorFlow data tensors).\n",
      "        y: Numpy array of target (label) data.\n",
      "            If the output layer in the model is named, you can also pass a\n",
      "            dictionary mapping the output name to a Numpy array.\n",
      "            `y` can be `None` (default) if feeding from\n",
      "            framework-native tensors (e.g. TensorFlow data tensors).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, it will default to 32.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling.\n",
      "        validation_data: tuple `(x_val, y_val)` or tuple\n",
      "            `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "            This will override `validation_split`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch').\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      "        initial_epoch: Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined.\n",
      "        validation_steps: Only relevant if `steps_per_epoch`\n",
      "            is specified. Total number of steps (batches of samples)\n",
      "            to validate before stopping.\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    # Raises\n",
      "        RuntimeError: If the model was never compiled.\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
